HBaseBulkImportXML
==================

An example of how to bulk import data from XML files into a HBase table.
<h3><b>hbase-bulk-import-XML-example</b></h3>

License
 
Apache licensed.

<br>
HBase gives random read and write access to your big data, but getting your big data into HBase can be a challenge. Using the API to put the data in works, but because it has to traverse HBase's write path (i.e. via the WAL and memstore before it is flushed to a HFile) it is slower than if you simply bypassed the lot and created the HFiles yourself and copied them directly into the HDFS.
<br>
Luckily HBase comes with bulk load capabilities, and this example demonstrates how they work. The HBase bulk load process consists of two steps:
<br>
<br>
1. HFile preparation via a MapReduce job, and <br>
2. Importing the HFile into HBase using LoadIncrementalHFiles.doBulkLoad<br>
<br>
The aim of the MapReduce job is to generate HBase data files (HFiles) from your input data using HFileOutputFormat. This output format writes out data in HBase's internal storage format so that they can be efficiently loaded into HBase.
<br>
HFileOutputFormat includes a convenience function, configureIncrementalLoad(), which automatically sets up a TotalOrderPartitioner based on the current region boundaries of a table.
<br>
There are two methods to import the generated HFiles into a HBase table. <br>
<br>
1. Command line tool called completeBulkLoad. <br>
2. Second is a programmatic approach which uses the LoadIncrementalHFiles.doBulkLoad method to load the HFiles generated by the previous MapReduce job into the given HBase table. This approach is used in this example.<br>

Output from Mapper class are ImmutableBytesWritable,KeyValue. These classes are used by the subsequent partitioner and reducer to create the HFiles.
<br>
The destination HBase table is called book.
<br>
There is no need to write your own reducer as the HFileOutputFormat.configureIncrementalLoad() as used in the driver code sets the correct reducer and partitioner up for you.
<br>
Check src/test/resource/test.xml to see demofile.
<br>
Open up a HBase shell and run the following to setup the table:
create 'book','bookFamily'
Before executing the Map Reduce program the hbase table should be created.

```
hadoop jar HBaseBulkLoadXml-0.0.1-SNAPSHOT-jar-with-dependencies.jar /inputDirectory/book.xml /outputDirectory/book.xml book
```
1. First argument- Input feed<br>
2. Second argument- Output directory<br>
3. Third argument HBase table- book<br>
